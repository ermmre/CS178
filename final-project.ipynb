{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1aa840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d21c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = './aclImdb/train'\n",
    "test_dir = './aclImdb/test'\n",
    "\n",
    "data_train = load_files(train_dir, categories=['pos','neg'], shuffle=True, encoding='utf-8')\n",
    "data_test = load_files(test_dir, categories=['pos','neg'], shuffle=True, encoding='utf-8')\n",
    "\n",
    "X_train, y_train = data_train.data, data_train.target\n",
    "X_test, y_test = data_test.data, data_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e9aeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(max_features=20000, ngram_range=(1,2))),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "nb_pipeline.fit(X_train, y_train)\n",
    "nb_preds = nb_pipeline.predict(X_test)\n",
    "\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_score(y_test, nb_preds))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, nb_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbef57c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nb = pd.DataFrame({\n",
    "    'review': X_test,\n",
    "    'true_label': y_test,\n",
    "    'predicted_label': nb_preds\n",
    "})\n",
    "\n",
    "misclassified_nb = df_nb[df_nb['true_label'] != df_nb['predicted_label']]\n",
    "print(f\"Number of misclassified NB reviews: {len(misclassified_nb)}\")\n",
    "\n",
    "for i, row in misclassified_nb.sample(5).iterrows():\n",
    "    print(f\"True label: {row['true_label']}, Predicted: {row['predicted_label']}\")\n",
    "    print(row['review'])\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5388c4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=20000, ngram_range=(1,2))),\n",
    "    ('lr', LogisticRegression(max_iter=2000))\n",
    "])\n",
    "\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "lr_preds = lr_pipeline.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, lr_preds))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, lr_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a427d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lr = pd.DataFrame({\n",
    "    'review': X_test,\n",
    "    'true_label': y_test,\n",
    "    'predicted_label': lr_preds\n",
    "})\n",
    "\n",
    "misclassified_lr = df_lr[df_lr['true_label'] != df_lr['predicted_label']]\n",
    "print(f\"Number of misclassified reviews: {len(misclassified_lr)}\")\n",
    "\n",
    "for i, row in misclassified_lr.sample(5).iterrows():\n",
    "    print(f\"True label: {row['true_label']}, Predicted: {row['predicted_label']}\")\n",
    "    print(row['review'])\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9807a7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=20000, ngram_range=(1,2))),\n",
    "    ('ensemble', VotingClassifier(\n",
    "        estimators=[\n",
    "            ('nb', MultinomialNB()),\n",
    "            ('lr', LogisticRegression(max_iter=2000))\n",
    "        ],\n",
    "        voting='hard'\n",
    "    ))\n",
    "])\n",
    "\n",
    "ensemble_pipeline.fit(X_train, y_train)\n",
    "\n",
    "ensemble_preds = ensemble_pipeline.predict(X_test)\n",
    "print(\"Ensemble Model Accuracy:\", accuracy_score(y_test, ensemble_preds))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, ensemble_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa53cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ensemble = pd.DataFrame({\n",
    "    'review': X_test,\n",
    "    'true_label': y_test,\n",
    "    'predicted_label': ensemble_preds\n",
    "})\n",
    "\n",
    "misclassified_ensemble = df_ensemble[df_ensemble['true_label'] != df_ensemble['predicted_label']]\n",
    "print(f\"Number of misclassified reviews in ensemble: {len(misclassified_ensemble)}\")\n",
    "\n",
    "for i, row in misclassified_ensemble.sample(5).iterrows():\n",
    "    print(f\"True label: {row['true_label']}, Predicted: {row['predicted_label']}\")\n",
    "    print(row['review'])\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9711a29",
   "metadata": {},
   "source": [
    "## Analysis of Misclassified Reviews\n",
    "\n",
    "### From examining the misclassified reviews produced by our models, we identified a couple reasons as to why our basic models are currently struggling to correctly label these reviews:\n",
    "\n",
    "#### 1. Word weighting in long reviews:\n",
    "For lengthy reviews, our models treat all words equally. When a review contains many positive words, these can outweigh negative words (or vice versa), causing the model to clearly misclassify. For example, a negative review with some early positive phrases might be labeled as positive because the total count of positive words dominates the prediction.\n",
    "\n",
    "#### 2. Lack of contextual understanding:\n",
    "Our models cannot capture context or sarcasm. As we read some of the reviews ourselves, there was a clear trend.\n",
    "\n",
    "Take this short snippet as an example: \n",
    "- \"This film is about a bunch of misfits who are supposed to be assigned to a task that is expected to fail miserably. The misfits pull together to successfully complete their mission.\"\n",
    "\n",
    "As you can see, this is very clearly a summary of the movie's plot, but our models see words like \"misfits,\" \"fail,\" and \"miserably\" and takes them completely out of context labeling them as weight for a negative prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
